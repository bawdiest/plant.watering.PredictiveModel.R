{
    "collab_server" : "",
    "contents" : "# Functions ---------------------------------------------------------------\n\ncomputeCostMulti <- function(X, y, theta) {\n  y <- as.vector(y);\n  #theta <- as.vector(theta, mode = 'numeric');\n  m <- length(y);\n  X <- data.matrix(X);\n  J <- X %*% (theta);\n  J <- ((t((J) - y ) %*% (J - y  ))) / (2*m);\n  #J <- ((t((X %*% theta) - y ) %*% ((X %*% theta) - y  ))) / (m);\n  return(J)\n}\n\ngradientDescentMulti <- function(X, y, alpha, num_iters) {\n  m <- length(y); # number of training examples\n  X <- data.matrix(X);\n  theta = data.matrix(rep(0.1,ncol(X)));\n  costH <- 0.1;\n  \n  for(iter in 1:num_iters) {\n    cost <- computeCostMulti(X,y,theta); #Get Error\n    factor <- (alpha/m);\n    htheta <- X %*% theta; #Compute Cost for each record\n    correctionFactor <- t(X) %*% (htheta - y); #Compute Error relative to each feature (Some features contribute to the total error more than others)\n    correctionFactor <- correctionFactor * factor; #Relativate Error to Learning factor\n    \n    theta <- theta - correctionFactor; #Correct theta\n    costH[iter] <- cost;\n  }\n  \n  r <- list(theta = theta, cost = costH);\n  return(r);\n}\n\nnormalizeMatrix <- function(x) { \n  \n  xMin <- apply(x,2,min);\n  xMax <- apply(x,2,max);\n  xDiff <- xMax - xMin;\n  xFactor <- diag((1)/(xMax-xMin));\n  \n  x <- t(as.matrix(t(x) - xMin));\n  x <- x %*% t(xFactor);\n  \n  attr(x,\"xMin\") <- xMin;\n  attr(x,\"xMax\") <- xMax;\n  xFactor <- 1/xDiff;\n  attr(x,\"xFactor\") <- xFactor;\n  return(x);\n};\n\nnormalizeVector <- function(x) { \n  attr(x, 'normalizeMin') <- min(x);\n  attr(x, 'normalizeMax') <- max(x);\n  xMin <- min(x);\n  xMax <- max(x);\n  \n  #x <- (x-min(x))/(max(x)-min(x));\n  #x <- (x-min(x))/(max(x));\n  xDiff <- xMax - xMin;\n  xFactor <- 1/xDiff;\n  \n  x <- x - xMin;\n  x <- x * xFactor;\n  \n  attr(x,\"yMax\") <- xMax;\n  attr(x,\"yMin\") <- xMin;\n  attr(x,\"yFactor\") <- xFactor;\n  \n  return(x);\n};\n\ntrainLinearModell <- function(X, y) {\n  X <- normalizeMatrix(X);\n  y <- normalizeVector(y);\n  \n  params = list(); \n  \n  params$xMin <- attributes(X)$xMin;\n  params$xMax <- attributes(X)$xMax;\n  params$xFactor <- attributes(X)$xFactor;\n  \n  params$yMin <- attributes(y)$normalizeMin;\n  params$yMax <- attributes(y)$normalizeMax;\n  params$yFactor <- attributes(y)$yFactor;\n  \n  gD <- gradientDescentMulti(X = X, y = y, alpha = 0.01, num_iters = 10000);\n  theta <- gD$theta;\n  \n  plot(gD$cost, xlab=\"Iteration\", ylab=\"Error\");\n  theta <- replace(theta, is.na(theta), 0)\n  \n  params$theta <- theta;\n  \n  return(params)\n}\n\npolyFeatures <- function(X, degree) {\n  c <- ncol(X)\n  for(j in 1:c) {\n    for (i in 2:degree) {\n      X <- cbind(X, X[,j] ^ i)\n      X <- cbind(X, X[,j] ^ 1/i)\n    }\n  }\n  return(X)\n}\n\n# computeCostMulti <- function(X, y, theta) {\n#   y <- as.vector(y);\n#   #theta <- as.vector(theta, mode = 'numeric');\n#   m <- length(y);\n#   X <- data.matrix(X);\n#   J <- X %*% (theta);\n#   J <- ((t((J) - y ) %*% (J - y  ))) / (2*m);\n#   #J <- ((t((X %*% theta) - y ) %*% ((X %*% theta) - y  ))) / (m);\n#   return(J)\n# }\n# \n# # gradientDescentMulti <- function(X, y, alpha, num_iters) {\n# #   m <- length(y);\n# #   X <- data.matrix(X);\n# #   theta <- vector(\"integer\", ncol(X));\n# #   theta <- as.vector(rep(0.1,ncol(X)), mode = 'numeric');\n# #   costH <- 0;\n# #   \n# #   for(iter in 1:num_iters) {\n# #     htheta <- X %*% theta;\n# #     thetaTmp <- 1;\n# #     for(row in 1:length(theta)) {\n# #       theta[row] = theta[row] - (alpha / m) * (t(htheta - y) %*% X[,row]);\n# #     }\n# #     #costH[iter] <- computeCostMulti(Xtest, ytest, theta);\n# #   }\n# #   r <- list(theta = theta, cost = costH);\n# #   return(r);\n# # }\n# \n# gradientDescentMulti <- function(X, y, alpha, num_iters) {\n#   m <- length(y); # number of training examples\n#   X <- data.matrix(X);\n#   theta = data.matrix(rep(0.1,ncol(X)));\n#   costH <- 0.1;\n#   \n#   for(iter in 1:num_iters) {\n#     cost <- computeCostMulti(X,y,theta); #Get Error\n#     factor <- (alpha/m);\n#     htheta <- X %*% theta;\n#     correctionFactor <- t(X) %*% (htheta - y);\n#     correctionFactor <- correctionFactor * factor;\n#     #factor <-  factor * cost; #Error by Learning Rate\n#     #f <- rep(as.numeric(factor), nrow(X)); # Multiplicate Error Rate to be able to vectorize function\n#     #nF <- t(X) %*% data.matrix(f); # Compute Error for each variable. (There are variables which influence the error more than others)\n#     #factror <- (factor) %*% t(X);\n#     \n#     theta <- theta - correctionFactor;\n#     costH[iter] <- cost;\n#     \n#     #if (computeCostMulti(X,y,theta) > cost) break \n#   }\n#   \n#   r <- list(theta = theta, cost = costH);\n#   return(r);\n# }\n\n\nX <- matrix(c(1,1,1,0.5,0.5,0.5,0.1,0.1,0.1), nrow = 3, ncol =3);\n\ny <- c(2,2,2);\n\nt <- gradientDescentMulti(X, y, 0.001, 100)",
    "created" : 1479826589634.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3172228817",
    "id" : "855EB6BE",
    "lastKnownWriteTime" : 1479827283,
    "last_content_update" : 1479827283226,
    "path" : "~/CloudStation/private/myProjects/plant.watering.PredictiveModel.R/gradientDescentMulti.R",
    "project_path" : "gradientDescentMulti.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}