---
title: "Blog"
author: "mikmak"
date: "13/12/2016"
output:
  html_document: default
  pdf_document: default
  word_document: default
---


```{r con.iRig, echo=FALSE}

rm(list=ls())

library("RMySQL");

# Create DB Connection ----------------------------------------------------

con.iRig <- dbConnect(MySQL(),
                 user = "read", 
                 host = "mikmak.cc",
                 password = "809913", 
                 db = "iRig");

source('gradientDescentMulti.R')

library('lubridate')
```

# Mit BigData und AI die Pflanzen bewässern
## Idee

Ein Algorithmus muss vorhersagen können wieviel Wasser meine Balkonpflanzen in den nächsten 24 Stunden brauchen werden.

## Warum BigData und AI und nicht handelsübliches Steuergerät

Warum so komplizierte Anlage bauen und nicht einfach eine handelsübliches Gerät kaufen, der mit einem Feuchtigkeitssensor ausgestattet ist. Hier sind einige Argumente:

- eine sensorgesteuerte Anlage hat meistens keine Internet Anbindung. D.h. wenn ich längere Zeit weg bin und es passiert etwas Unvorgesehnbares, kann ich nicht wissen ob die Anlage noch mit Strom versorgt wird (meine Anlage ist an eine Batterie angeschlossen welche von einem Solar Pannel aufgeladen wird), ob der Feuchtigkeitssensor noch richtig positioniert ist und nicht vom Wind umgeworfen wurde und so falsche Daten an die Steuereinheit sendet. (Es ist mir früher schon passiert).

- Handelsübliche Anlagen sind so programmiert, dass diese zu einem bestimmten Zeitpunkt die Sensortwerte auswerten und entscheiden ob die Pumpe gestartet werden soll oder nicht. Z.B. um 17 Uhr meldet der Sensor, dass die Erde trocken ist und es Wasser braucht. Um 17:05 fängt es zu regnen an. So hat die Anlage umsonst das Wasser und Strom verbraucht.

- Skalleneffekte. Eine an die Cloud angeschlossene Anlage benötigt keinen hochqualitativen (und teueren) Sensor. Beim Bau jeder weieter Anlage sinken die Hardware- und somit die Gesamtkosten.

## Lösung im Detail

### Algorithm

Es gibt grundsätzlich zwei Arten von Algorythmen: 

- Überwachtes Lernen: hier verwendet man historische Daten um den Rechner zu "trainieren" anhand der Input Daten den richtigen Output zu berechnen. z.B. Regressionsanalysen, Neueronale Netze

- Unüberwachtes Lernen: hier lässt man den Rechner die Daten analysieren und gewisse gesetzesmässigkeiten oder Muster zu erkennen. Beispiele sind: Korrelationsanalyse, Clustering. 
Für mein Vorhaben eignen sich die Algorithmen für überwachtes Lernen am Besten. Dabei habe ich dem Tipp eines Stanford Professors gefolgt und habe mit dem einfachsten von Allen begonnen, mit der linearer Regression.

### Lineare Regression
Alle Algorithmen vom Typ Überwachtes Lernen kann man in Form einer mathematischer Funktion $y = f(X)$ darstellen, wo X eine oder mehrere Input Variablen sind und y ist die sogenannte Target Variable, also der Wert, den wir voraussagen möchten.

Die mathematische Funktion für die lineare Regression sieht so aus:

$$y = \sum_{i=1}^n (X_i*\Theta_i)$$

Auch hier steht X für Input Variablen, in meinem Fall sind es: Höchsttemperatur in C in den letzten 24 Stunden, durchschnittlicher Luftdruck in den letzten 24 Stunden usw. y ist die Wassermenge in ml, die eine Pflanze aktuell benötigt. Um y zu berechnen, muss man die Input Parameter X mit den Modell Parametern $\theta$ multiplizieren und die Resultate aufsummieren.

Tönt einfach, ist es auch, mann muss nur passende Parameter $\theta$ finden. Man nennt es das Modell trainieren. Dazu verwende ich die "Methode der kleinsten Quadrate".

1. Dabei vergibt man allen $\theta$ Parametern einen Initialwert (i.d.R. 0.1) und 
2. man berechnet den Zielparameter $y$ für die historischen Daten. Die Summe der Differenzen zwischen dem errechneten Wert $y_{ber}$ und dem wahren historischen Wert $y_{wahr}$ im Quadrat $\sum (\sqrt[2]{(y_{ber} - y_{wahr})^{2}})$ (deswegen Methode der kleinsten Quadrate) ist der korrektur Faktor. 
3. Danach korrigiert man die Parameter $\theta$ um den Korrektur Faktor. Damit nicht alle Thetas um den selben Wert korrigiert werden, multipliziert man den Korrekturfaktor mit dem jeweiligen Wert $x$. Anschliessend
4. wiederholt man die Schritte 1-3 x-fach bis der Korrekturfaktor sich nicht mehr wesentlich ändert. In meinem Fall haben 300 Iterationen gereicht.

OK, es ist doch eine zwar richtige, aber doch vereinfachte Erklärung der Methode der kleinsten Quadrate. Wer mehr wissen möchte, empfehle ich den Wikipedia Artikel.

Viele Programmiersprachen der 4. Generation (4GL) entwickelt für die statistische Datenverarbeitung wie R, MATLAB, SPSS usw. bieten die Funktionen zur optimierung der linearen Regressionen bereits in der Standardauslieferung.

In R verwendet man dazu die Funktion $lm()$
```{r echo=TRUE, eval=FALSE}
lm(y ~ x + 1)
```

### Daten

Gemäss einer Studie von Stanford University sind die Trainings Daten wichtiger, als die Wahl des Algorythumus.

Die Daten kann man entweder selber sammeln, kaufen oder die Daten der öffentlichen Insitutionen verwenden (Stichwort "Open Data").

Für meinen Fall verwende ich 
- öffentliche Meteo Daten von der MeteoSuisse auf http://www1.ncdc.noaa.gov/pub/data/noaa/ und 
- den von mir gesammelten Bewässerungslog (wie lange ich an welchem Tag die Pflanzen mit der Gardena Anlage bewässert habe)

Meinen Bewässerungslog habe ich mit der Variable $y$ bezeichnet.

```{r echo=FALSE}
sql <- paste('SELECT MsgID, msgv1, TmStp FROM sysLog WHERE msgID = "109" AND sysID = "79cf6c22-dcc6-11e5-8e77-00113217113f"')
query <- dbSendQuery(con.iRig, statement = sql);
e_pumpLog <- fetch(query)
dbDisconnect(con.iRig)
```


```{r echo=FALSE}
# Derive Time Characteristics ---------------------------------------------
e_pumpLog[3] <- as.POSIXct(e_pumpLog[,3] , origin="1970-01-01")
t_pumpLog <- e_pumpLog[order(e_pumpLog$TmStp),]
t_pumpLog <- cbind(t_pumpLog, round(t_pumpLog$TmStp, units = "hours"))
names(t_pumpLog)[ncol(t_pumpLog)] <- "TimeStpRnd"
t_pumpLog <- cbind(t_pumpLog, as.Date(t_pumpLog$TimeStpRnd))
names(t_pumpLog)[ncol(t_pumpLog)] <- "DateRnd"

# Fill missing logs with zeroes in 2014 (For 2016 there are record --------
minDate <- min(t_pumpLog$DateRnd)
maxDate <- max(t_pumpLog$DateRnd[year(t_pumpLog$DateRnd) == 2014])
missingValues <- data.frame("DateRnd" = seq(from = as.Date(minDate), to = as.Date(maxDate), by = "days")
                 , "msgv1" = 0)
t_pumpLog <- rbind(t_pumpLog[, c("DateRnd", "msgv1")], missingValues)

# Calculate the sum by day and set pumping time to 07:00 PM ---------------
t_pumpLog <- aggregate(as.integer(t_pumpLog$msgv1), by=list(DateRnd = t_pumpLog$DateRnd), FUN=sum)
t_pumpLog <- cbind(t_pumpLog, paste(t_pumpLog$DateRnd, "19:00:00", sep = " "))
names(t_pumpLog)[ncol(t_pumpLog)] <- "TimeStpRnd"
t_pumpLog$TimeStpRnd <- parse_date_time(t_pumpLog$TimeStpRnd, c('ymd HMS'))

# Rename the Result Field -------------------------------------------------
names(t_pumpLog)[names(t_pumpLog) == "x"] <- "y"

# Remove Standard Values --------------------------------------------------
#t_pumpLog <- t_pumpLog[year(t_pumpLog[,"DateRnd"]) == "2014", ]
t_pumpLog <- t_pumpLog[!t_pumpLog[,"y"]==0,] #Remove standard Value 
t_pumpLog <- t_pumpLog[!t_pumpLog[,"y"]==480,] #Remove standard Value 

# Write Data to Propagation Layer -----------------------------------------
p_pumpLog <- t_pumpLog

# Display the Result ------------------------------------------------------
plot(p_pumpLog[, c("TimeStpRnd", "y")], type = "h", col = "darkblue")
print(summary((p_pumpLog[,"y"])))
```


#### Wetterdaten einlesen und verarbeiten
```{r echo=TRUE}
e_weatherData2014 <- read.csv("~/CloudStation/private/myProjects/plant.watering.PredictiveModel.R/data/shopdwhdata_2YH_WAE.csv", sep=";", stringsAsFactors=FALSE)
e_weatherData2016 <- read.csv("~/CloudStation/private/myProjects/plant.watering.PredictiveModel.R/data/shopdwhdata_0YH_WAE.csv", sep=";", stringsAsFactors=FALSE)

t_weatherData <- rbind(e_weatherData2014, e_weatherData2016)
t_weatherData <- cbind(t_weatherData, paste(t_weatherData$date, t_weatherData$time, sep = " "))
names(t_weatherData)[ncol(t_weatherData)] <- "TimeStp"

t_weatherData$TimeStp <- parse_date_time(t_weatherData$TimeStp, c('dmy HM'))

p_weatherData <- t_weatherData

# Output --------------------------------------------------------------------
plot(p_weatherData[, c("TimeStp", "tre200bx")], type = "l", col="grey")
print(summary(p_weatherData))
```

#### Datenset vorbereiten
* Input
    * Wetter vor 24 Stunden
    * Wetter vor 48 Stunden
    * Bewesserungstageszeit (Abend/Morgen, Nacht, Tag)
    * Wetterveränderung seit 24 Stunden (Temperatur, Luftdruck, Niederschlagsmenge)
* Output
    * Wasserkonsum in den nächsten 24 Stunden
    
## Pump Duration in the last 24 hours
```{r echo=TRUE}
# Get PumpDuration for last 24 hours -----
t_dataSet <- p_pumpLog

lv_pumpDurationInLast24h <- apply(t_dataSet, 1, function(x) {
  lv_date <- as.POSIXct(x["TimeStpRnd"]) - days(1)
  lv_date_24h <- as.POSIXct(x["TimeStpRnd"]) - hours(49)
  lv_d <- t_dataSet[t_dataSet$TimeStp >= lv_date_24h & t_dataSet$TimeStp < lv_date,c(2)] # Get PumpDuration for last 48 hours
  lv_y <- sum(lv_d)
    lv_y <- cbind(lv_y, 1) #Add column with ones to replace it in the next step with dates
    lv_y[ncol(lv_y)] <- as.POSIXct(lv_date, origin="1970-01-01", tz="GMT" )
  return(lv_y)
})
a_pumpDurationInLast24h <- as.data.frame((unlist(t(lv_pumpDurationInLast24h))))
a_pumpDurationInLast24h[,ncol(a_pumpDurationInLast24h)] <- as.POSIXct(a_pumpDurationInLast24h[,ncol(a_pumpDurationInLast24h)], origin = "1970-01-01", tz = "GMT") + hours(2)
names(a_pumpDurationInLast24h) <- c("P24h","TimeStpP24h")

plot(a_pumpDurationInLast24h[, c("TimeStpP24h", "P24h")], type = "h", col = "grey")
#lv_test <- merge(t_dataSet, lv_pumpDurationInLast24h, by.x = "TimeStpRnd", by.y = "TimeStpP24h", all.x = TRUE)
```

```{r echo=TRUE}
# Get Sums and Means for WeatherData of last 24 hours ----

a_weatherData <- p_weatherData

 a_sumsANDmeans <- apply(t_dataSet, 1, function(x) {
  lv_date <- as.POSIXct(x["TimeStpRnd"]) - days(1)
  lv_date_24h <- as.POSIXct(x["TimeStpRnd"]) - days(2)
  lv_d <- a_weatherData[a_weatherData$TimeStp >= lv_date_24h & a_weatherData$TimeStp <= lv_date,] # Get weather Data for last 48 hours
  
  lv_y <- mean(lv_d$tre200b0)
  lv_y <- cbind(lv_y, mean(lv_d$tre200bn))
  lv_y <- cbind(lv_y, mean(lv_d$tre200bx))
  lv_y <- cbind(lv_y, mean(lv_d$ure200b0))
  lv_y <- cbind(lv_y, sum(lv_d$rre150b0))
  lv_y <- cbind(lv_y, mean(lv_d$prestab0))
  lv_y <- cbind(lv_y, 1) #Add column with ones to replace it in the next step with dates
  lv_y[ncol(lv_y)] <- as.POSIXct(lv_date, origin="1970-01-01", tz="GMT" )

  return(lv_y)
})
 a_sumsANDmeans <- as.data.frame(t(unlist(a_sumsANDmeans)))
 a_sumsANDmeans[,ncol(a_sumsANDmeans)] <- as.POSIXct(a_sumsANDmeans[,ncol(a_sumsANDmeans)], origin = "1970-01-01", tz = "GMT") + hours(2)
 names(a_sumsANDmeans) <- c("tre200b0Mean", "tre200bnMean", "tre200bxMean", "ure200b0Mean", "rre150b0Sum", "prestab0Mean", "V7")
 
 plot( a_sumsANDmeans[, c("V7", "tre200bxMean")], type = "l", col="grey")
 
``` 
 
```{r echo=TRUE}
# Merge together PumpDuration and WeatherData ------ 
#t_dataSet <- merge(t_dataSet, a_weatherData, by.y = "TimeStp-24", by.x = "TimeStpRnd", suffixes = c(".m",".24"))
t_dataSet <- merge(t_dataSet, a_weatherData, by.y = "TimeStp", by.x = "TimeStpRnd")
t_dataSet <- merge(x = t_dataSet, y = a_sumsANDmeans, by.y = "V7", by.x = "TimeStpRnd", all.x = TRUE, all.y = TRUE)
t_dataSet <- merge(x = t_dataSet, y = a_pumpDurationInLast24h, by.y = "TimeStpP24h", by.x = "TimeStpRnd", all.x = TRUE, all.y = TRUE)

# Drop unneccessairy variables --------------------------------------------

#columnsToDrop <- names(t_dataSet) %in% grep("[.y]$", names(t_dataSet), value=TRUE)
#t_dataSet <- t_dataSet[,!columnsToDrop]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% c("TimeStpRnd"))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^time*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^date*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^stn*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^fu*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^TimeStp*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% grep("^dkl*", names(t_dataSet), value=TRUE))]
t_dataSet <- t_dataSet[, !(names(t_dataSet) %in% c("DateRnd"))]

# Prepare DataSet for machine learning --------------------------------------------
t_dataSet <- na.omit(t_dataSet)

a_dataSet <- data.matrix(t_dataSet)
a_dataSet <- as.data.frame(a_dataSet)

a_dataSet <- polyFeatures(a_dataSet,3)
a_dataSet <- a_dataSet[, !(names(t_dataSet) %in% grep("^x*2", names(a_dataSet), value=TRUE))] #No X's in dataset!!!

# Double DataSet ---------
a_dataSet <- rbind(a_dataSet, a_dataSet)

 # Train models --------------------------------------------
sample <- sample.int(nrow(a_dataSet), nrow(a_dataSet)*0.8) #nicht weniger als 80%!!!
a_dataSetTrain <- a_dataSet[sample,]
 model <- lm(`y` ~ . + 1, data = a_dataSetTrain)

# Output --------
 a_dataSetTest <- a_dataSet[-c(sample),-c(1)]
 a_dataSetTest <- cbind(rep(1,nrow(a_dataSetTest)), a_dataSetTest)
test <- predict(model, newdata = a_dataSetTest)
test <- cbind(test,a_dataSet[-c(sample),c(1)])
train <- predict(model)
train <- cbind(train,a_dataSet[c(sample),c(1)])
#d <- as.data.frame(d)
#names(d) <- c("Predicted", "Real")

layout(matrix(c(1,2),2,1)) # optional 4 graphs/page 
plot(train[,1], train[,2], pch = 16, cex = 0.8, col = "blue")
plot(test[,1], test[,2], pch = 16, cex = 0.8, col = "blue")



summary(model)
```

```{r echo=TRUE}
#View(cbind(predict(model,a_dataSet),a_dataSet$y))
```
#### Analyse des Models
```{r echo=TRUE}
# diagnostic plots 
#layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
#plot(model)
```

